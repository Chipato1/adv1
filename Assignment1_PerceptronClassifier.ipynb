{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP47590: Advanced Machine Learning\n",
    "# Assignment 1: Implementing Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Student 1 Name: Carl Fabian Winkler\n",
    "- Student 1 Number: 20207528\n",
    "- Student 2 Name: David Moreno Boras\n",
    "- Student 2 Number: 21200646"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.model_selection import train_test_split\\n\\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted, check_random_state\\nfrom sklearn.utils.multiclass import unique_labels\\nfrom sklearn import preprocessing\\nfrom sklearn import metrics\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.utils import resample'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted, check_random_state\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: The Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the PerceptronClassifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, n_in, n_out, activation = 'Sigmoid', init = 'Xavier'):\n",
    "        self.activation = activation\n",
    "        # XW + b = y ; We input more than one sample per pass...\n",
    "        \n",
    "        self.weights = np.zeros((n_in, n_out))\n",
    "        self.biases = np.zeros((n_out))\n",
    "        if init == 'Xavier':\n",
    "            var = np.sqrt(6.0 / (n_in + n_out))\n",
    "            for i in range(n_in):\n",
    "                for j in range(n_out):\n",
    "                      self.weights[i,j] = np.float32(np.random.uniform(-var, var))\n",
    "        print(\"Weights:\", self.weights.shape)\n",
    "        print(self.weights) \n",
    "        \n",
    "    def getWeights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"X:\", x.shape)\n",
    "        print(x)\n",
    "        print(\"Weights:\", self.weights.shape)\n",
    "        print(self.weights)\n",
    "        z = x @ self.weights + self.biases\n",
    "        \n",
    "        if self.activation == 'Sigmoid':\n",
    "            out = 1 / (1 + np.exp(-z))\n",
    "        elif self.activation == 'ReLu':\n",
    "            out = np.maximum(z, 0)\n",
    "        elif self.activation == 'TanH':\n",
    "            out = np.tanh(z)\n",
    "        else:\n",
    "            out = z\n",
    "        \n",
    "        self.cache = (x, z)\n",
    "        \n",
    "        return out    \n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        inputs, z = self.cache\n",
    "        weight = self.weights\n",
    "        bias = self.biases\n",
    "        \n",
    "        if self.activation == 'Sigmoid':\n",
    "            d_act = d_out * (1 / (1 + np.exp(-z))) * (1 - 1 / (1 + np.exp(-z)))\n",
    "        elif self.activation == 'ReLu':\n",
    "            d_act = d_out * (z > 0)\n",
    "            \n",
    "        elif self.activation == 'TanH':\n",
    "            d_act = d_out * np.square(z)\n",
    "        else:\n",
    "            d_act = z\n",
    "            \n",
    "        d_inputs = d_act @ weight.T\n",
    "        d_weight = inputs.T @ d_act\n",
    "        d_bias = d_act.sum(axis=0) \n",
    "        \n",
    "        self.d_w = d_weight\n",
    "        self.d_b = d_bias\n",
    "        \n",
    "        return d_inputs, d_weight, d_bias\n",
    "    \n",
    "    def update_gd_params(self, lr):\n",
    "        self.weights = self.weights - lr * self.d_w\n",
    "        self.biases = self.biases - lr * self.d_b\n",
    "\n",
    "class PerceptronClassifier(BaseEstimator, ClassifierMixin, ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    Attributes\n",
    "    ----------\n",
    "    Notes\n",
    "    -----\n",
    "    See also\n",
    "    --------\n",
    "    Examples\n",
    "    --------\n",
    "    \"\"\"\n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self, in_dim, out_dim, hidden_units, n_layers, activation = 'Sigmoid', \n",
    "                 learning_rate = 0.01, weight_decay = 0, epochs = -1, regularisation = 'L2'):\n",
    "\n",
    "        \"\"\"Setup a Perceptron classifier .\n",
    "        Parameters\n",
    "        ----------\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"     \n",
    "        \n",
    "        self.layers = []\n",
    "        self.lr = learning_rate\n",
    "        self.regularisation = regularisation\n",
    "        \n",
    "        self.layers.append(Layer(in_dim, hidden_units, activation, 'Xavier'))\n",
    "        for l in range(n_layers):\n",
    "            self.layers.append(Layer(hidden_units, hidden_units, activation, 'Xavier'))\n",
    "        self.layers.append(Layer(hidden_units, out_dim, activation, 'Xavier'))\n",
    "        \n",
    "        print(\"Layers:\", len(self.layers))\n",
    "        \n",
    "        # Initialise class variabels\n",
    "    def forward(self, X):\n",
    "        out = self.layers[0].forward(X)\n",
    "        for layer in self.layers[1:]:\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "                \n",
    "    def backward(self, in_grad):\n",
    "        i = len(self.layers) - 2 \n",
    "        # d_inputs, _, _ = lay.backward(in_grad)\n",
    "        next_grad = self.layers[i+1].backward(in_grad)\n",
    "        while i >= 1:\n",
    "            next_grad = self.layers[i].backward(next_grad)\n",
    "            i -= 1\n",
    "        \n",
    "    def l2_loss(self, y_hat, pred):\n",
    "        # totalSum = 0\n",
    "        # for layer in self.layers:\n",
    "        #     totalSum = totalSum + np.sum(np.sum(layer.getWeights())\n",
    "        return -y_hat-pred\n",
    "\n",
    "    def loss(self, y_hat, pred):\n",
    "        return -y_hat-pred\n",
    "        \n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y, epochs = 30):\n",
    "        # WRITE CODE HERE\n",
    "        for i in range(epochs):\n",
    "            out = self.forward(X)\n",
    "            if (self.regularisation == 'L2'):\n",
    "                grad = self.l2_loss(y, out)\n",
    "            else:\n",
    "                grad = self.loss(y, out)\n",
    "                \n",
    "            # Backpropagation\n",
    "            self.backward(grad)\n",
    "            \n",
    "            # Update weights and biases\n",
    "            for layer in self.layers:\n",
    "                layer.update_gd_params(self.lr)\n",
    "        return\n",
    "    \n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    # The predict_proba function to make a set of predictions for a set of query instances. This returns a set of class distributions.\n",
    "    def predict_proba(self, X):\n",
    "        tmp = self.forward(X)\n",
    "        sum1 = tmp.sum(axis = 1)\n",
    "        out = X.T / sum1\n",
    "        out = out.T\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Diabethic Retinopathy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,1],[2,2]])\n",
    "\n",
    "\n",
    "D = 2\n",
    "N = 2\n",
    "H = 1\n",
    "\n",
    "\n",
    "weights = np.ones((2,1))\n",
    "biases = np.ones((1))  \n",
    "              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,1],[2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1,1,2],[2,1,3]])\n",
    "sum1 = x.sum(axis = 1)\n",
    "x = x.T / sum1\n",
    "x = x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (2, 1)\n",
      "[[-0.72654527]\n",
      " [ 0.4850854 ]]\n",
      "Weights: (1, 1)\n",
      "[[1.43463349]]\n",
      "Weights: (1, 1)\n",
      "[[0.60716546]]\n",
      "Layers: 3\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,1],[2,2]])\n",
    "y = np.ones(1)\n",
    "clf = PerceptronClassifier(2, 1, 1, 1, regularisation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (2, 2)\n",
      "[[1 1]\n",
      " [2 2]]\n",
      "Weights: (2, 1)\n",
      "[[-0.72654527]\n",
      " [ 0.4850854 ]]\n",
      "X: (2, 1)\n",
      "[[0.43992662]\n",
      " [0.38156291]]\n",
      "Weights: (1, 1)\n",
      "[[1.43463349]]\n",
      "X: (2, 1)\n",
      "[[0.65274643]\n",
      " [0.63353284]]\n",
      "Weights: (1, 1)\n",
      "[[0.60716546]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47510/533334978.py:45: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  d_act = d_out * (1 / (1 + np.exp(-z))) * (1 - 1 / (1 + np.exp(-z)))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47510/3927561159.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# clf.predict(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_47510/533334978.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, epochs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;31m# Update weights and biases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_47510/533334978.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, in_grad)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mnext_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mnext_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_47510/533334978.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, d_out)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0md_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0md_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_act\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0md_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0md_act\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0md_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_act\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 3)"
     ]
    }
   ],
   "source": [
    "clf.fit(x, y)\n",
    "# clf.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the perfomrance of the perceptron classifier on the daibetic retinopathy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(outpur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 & 4: Add Different Activations & Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reimplement the PerceptronClassifier class adding an activation function option and L2 regularisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronClassifier2(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self):\n",
    "\n",
    "        \"\"\"Setup a Perceptron classifier .\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"     \n",
    "\n",
    "        # Initialise ranomd state if set\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Initialise class variabels\n",
    "\n",
    "        \n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # WRITE CODE HERE\n",
    "        \n",
    "\n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict(self, X):\n",
    "\n",
    "        # WRITE CODE HERE\n",
    "    \n",
    "    # The predict_proba function to make a set of predictions for a set of query instances. This returns a set of class distributions.\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        # WRITE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and explore it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Reflect on the Performance of the Different Models Evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform hyper-parameter tuning and evaluate models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Diabetic Retiniphaty dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_af = pd.read_csv('messidor_features.csv', na_values='?', index_col = 0)\n",
    "diabetic_af.head()\n",
    "y = diabetic_af.pop('Class').values\n",
    "x_raw = diabetic_af.values\n",
    "print(\"Features: \", x_raw[0:2])\n",
    "print(\"Class: \", y[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and predict using our classifier\n",
    "\n",
    "With a single split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_raw, y, shuffle=True, train_size = 0.7)\n",
    "clf = PerceptronClassifier(len(x_train[0]), 1, )\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "accuracy = metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search\n",
    "\n",
    "Do grid search with the train set, use the test set for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_folds = 5\n",
    "param_grid ={'activation': ['Sigmoid', 'ReLu', 'TanH'], 'regularisation':['None', 'L2']}\n",
    "\n",
    "# Perform the search\n",
    "tuned_perceptron = GridSearchCV(PerceptronClassifier(), \\\n",
    "                            param_grid, cv=cv_folds, verbose = 2, \\\n",
    "                            n_jobs = -1)\n",
    "cross_val_score(clf, x_train, y_train, cv=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
