{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP47590: Advanced Machine Learning\n",
    "# Assignment 1: Implementing Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Student 1 Name: Carl Fabian Winkler\n",
    "- Student 1 Number: 20207528\n",
    "- Student 2 Name: David Moreno Boras\n",
    "- Student 2 Number: 21200646"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.model_selection import train_test_split\\n\\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted, check_random_state\\nfrom sklearn.utils.multiclass import unique_labels\\nfrom sklearn import preprocessing\\n\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.utils import resample'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted, check_random_state\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: The Perceptron Classifier\n",
    "Define the PerceptronClassifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, n_in, n_out, activation = 'Sigmoid', init = 'Xavier', output_layer = False):\n",
    "        self.activation = activation\n",
    "        # XW + b = y ; We input more than one sample per pass...\n",
    "        \n",
    "        self.output_layer = output_layer\n",
    "        self.weights = np.zeros((n_in, n_out))\n",
    "        self.biases = np.zeros((n_out))\n",
    "        if init == 'Xavier':\n",
    "            var = np.sqrt(6.0 / (n_in + n_out))\n",
    "            for i in range(n_in):\n",
    "                for j in range(n_out):\n",
    "                      self.weights[i,j] = np.float32(np.random.uniform(-var, var))\n",
    "                        \n",
    "        self.d_w = np.zeros(weights.shape)\n",
    "        self.d_b = np.zeros(biases.shape)\n",
    "        #print(\"Weights:\", self.weights.shape)\n",
    "        #print(self.weights) \n",
    "        \n",
    "    def getWeights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"print(\"X:\", x.shape)\n",
    "        print(x)\n",
    "        print(\"Weights:\", self.weights.shape)\n",
    "        print(self.weights)\"\"\"\n",
    "        z = x @ self.weights + self.biases\n",
    "        #print(\"Weights:\", self.weights.shape)\n",
    "        if self.activation == 'Sigmoid':\n",
    "            out = 1 / (1 + np.exp(-z))\n",
    "        elif self.activation == 'ReLu':\n",
    "            out = np.maximum(z, 0)\n",
    "        elif self.activation == 'TanH':\n",
    "            out = np.tanh(z)\n",
    "        else:\n",
    "            out = z\n",
    "        \n",
    "        self.cache = (x, z)\n",
    "        \n",
    "        return out    \n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        inputs, z = self.cache\n",
    "        weight = self.weights\n",
    "        bias = self.biases\n",
    "        #print(\"DOUT:\", d_out)\n",
    "        #print(\"shape\", d_out.shape)\n",
    "        if self.output_layer:\n",
    "            d_act = z\n",
    "        if self.activation == 'Sigmoid':\n",
    "            d_act = d_out * (1 / (1 + np.exp(-z))) * (1 - 1 / (1 + np.exp(-z)))\n",
    "        elif self.activation == 'ReLu':\n",
    "            d_act = d_out * (z > 0)\n",
    "            \n",
    "        elif self.activation == 'TanH':\n",
    "            d_act = d_out * np.square(z)\n",
    "        else:\n",
    "            d_act = z\n",
    "        #print(\"d_act:\", d_act)\n",
    "        #print(\"shape\", d_act.shape)    \n",
    "        d_inputs = d_act @ weight.T\n",
    "        self.d_w = inputs.T @ d_act\n",
    "        self.d_b = d_act.sum(axis=0) \n",
    "        \n",
    "        return d_inputs#, self.d_w, self.d_b\n",
    "    \n",
    "    def update_gd_params(self, lr):\n",
    "        #print(\"Weight upd: \", self.d_w)\n",
    "        self.weights = self.weights - lr * self.d_w\n",
    "        self.biases = self.biases - lr * self.d_b\n",
    "\n",
    "class PerceptronClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    Attributes\n",
    "    ----------\n",
    "    Notes\n",
    "    -----\n",
    "    See also\n",
    "    --------\n",
    "    Examples\n",
    "    --------\n",
    "    \"\"\"\n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self, in_dim, out_dim, hidden_units, n_layers, activation = 'Sigmoid', \n",
    "                 learning_rate = 0.01, weight_decay = 0, epochs = 30, regularisation = 'L2'):\n",
    "\n",
    "        \"\"\"Setup a Perceptron classifier .\n",
    "        Parameters\n",
    "        ----------\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"     \n",
    "        self.epochs = epochs\n",
    "        self.layers = []\n",
    "        self.lr = learning_rate\n",
    "        self.regularisation = regularisation\n",
    "        \n",
    "        self.layers.append(Layer(in_dim, hidden_units, activation, 'Xavier'))\n",
    "        for l in range(n_layers):\n",
    "            self.layers.append(Layer(hidden_units, hidden_units, activation, 'Xavier'))\n",
    "        self.layers.append(Layer(hidden_units, out_dim, activation, 'Xavier', output_layer = True))\n",
    "        \n",
    "        print(\"Layers:\", len(self.layers))\n",
    "        \n",
    "        # Initialise class variabels\n",
    "    def forward(self, X):\n",
    "        out = self.layers[0].forward(X)\n",
    "        for layer in self.layers[1:]:\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "                \n",
    "    def backward(self, in_grad):\n",
    "        i = len(self.layers) - 2 \n",
    "        # d_inputs, _, _ = lay.backward(in_grad)\n",
    "        next_grad = self.layers[i+1].backward(in_grad)\n",
    "        while i >= 0:\n",
    "            next_grad = self.layers[i].backward(next_grad)\n",
    "            i -= 1 \n",
    "            \n",
    "    def loss_back(self, ground, pred):\n",
    "        l2_loss = np.sum(np.power(ground-np.squeeze(pred, axis=1), 2)) / len(ground)\n",
    "        upstream_grad = -np.expand_dims(ground-np.squeeze(pred),axis=1)\n",
    "        #print(\"Mean Loss:\", l2_loss)\n",
    "        return l2_loss, upstream_grad\n",
    "            \n",
    "    def l2_loss_back(self, y_hat, pred):\n",
    "        return -np.expand_dims(y_hat-np.squeeze(pred),axis=1)\n",
    "        \n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y):\n",
    "        # WRITE CODE HERE\n",
    "        \n",
    "        loss_curve = []\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            out = self.forward(X)\n",
    "            #print(\"Prediction:\",out)\n",
    "            \n",
    "            l2_loss, grad = self.loss_back(y, out)\n",
    "            loss_curve.append(l2_loss)\n",
    "            # Backpropagation\n",
    "            self.backward(grad)\n",
    "            \n",
    "            # Update weights and biases\n",
    "            for layer in self.layers:\n",
    "                layer.update_gd_params(self.lr)\n",
    "        return\n",
    "    \n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    # The predict_proba function to make a set of predictions for a set of query instances. This returns a set of class distributions.\n",
    "    def predict_proba(self, X):\n",
    "        tmp = self.forward(X)\n",
    "        sum1 = tmp.sum(axis = 1)\n",
    "        out = X.T / sum1\n",
    "        out = out.T\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Diabethic Retinopathy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "diabetic_af = pd.read_csv('messidor_features.csv', na_values='?', index_col = 0)\n",
    "diabetic_af.head()\n",
    "y = diabetic_af.pop('Class').values\n",
    "x_raw = diabetic_af.values\n",
    "x_norm = NormalizeData(x_raw)\n",
    "y_numbers = list([int(x[2]) for x in y])\n",
    "y_numbers = np.array(y_numbers)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_norm, y_numbers, shuffle=False, train_size = 0.6)\n",
    "\n",
    "x_train = NormalizeData(x_train)\n",
    "x_test = NormalizeData(x_test)\n",
    "y_train = NormalizeData(y_train)\n",
    "y_test = NormalizeData(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the model\n",
    "To make sure that our model is abled to learn and that it works properly we overfit on a very small part of the data. In this case we reach an accuracy of 100 %. The model therefore clearly has the ability to learn and the capacity to learn and even ovefit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 3\n",
      "Accuracy of the overfitting is:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_overfit = x_train[0:15]\n",
    "y_overfit = y_train[0:15]\n",
    "\n",
    "clf = PerceptronClassifier(len(x_train[0]), 1, 30, 1, learning_rate=0.5, epochs=10000)\n",
    "clf.fit(x_overfit, y_overfit)\n",
    "y_pred = clf.predict(x_overfit)\n",
    "y_pred_binary = [1 if x >= 0.5 else 0 for x in y_pred]\n",
    "accuracy = metrics.accuracy_score(y_pred_binary, y_overfit)\n",
    "print('Accuracy of the overfitting is: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Baseline for the data\n",
    "To have a model that the perceptron can be compared to a baseline model is created here. Therefore a perceptron model of the sklearn library is trained to the data. We are trying to reach or even be better than this baseline model in terms of accuracy with the perceptron implemented here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 2\n"
     ]
    }
   ],
   "source": [
    "#(self, in_dim, out_dim, hidden_units, n_layers,\n",
    "x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([0,1,1,0])\n",
    "clf = PerceptronClassifier(2, 1, 5, 0, regularisation='None', learning_rate = 10,activation='Sigmoid')\n",
    "clf.fit(x, y,epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 & 4: Add Different Activations & Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reimplement the PerceptronClassifier class adding an activation function option and L2 regularisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronClassifier2(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self):\n",
    "\n",
    "        \"\"\"Setup a Perceptron classifier .\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"     \n",
    "\n",
    "        # Initialise ranomd state if set\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Initialise class variabels\n",
    "\n",
    "        \n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # WRITE CODE HERE\n",
    "        \n",
    "\n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict(self, X):\n",
    "\n",
    "        # WRITE CODE HERE\n",
    "    \n",
    "    # The predict_proba function to make a set of predictions for a set of query instances. This returns a set of class distributions.\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        # WRITE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and explore it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Reflect on the Performance of the Different Models Evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform hyper-parameter tuning and evaluate models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Diabetic Retiniphaty dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  [[1.0000000e+00 2.2000000e+01 2.2000000e+01 2.2000000e+01 1.9000000e+01\n",
      "  1.8000000e+01 1.4000000e+01 4.9895756e+01 1.7775994e+01 5.2709200e+00\n",
      "  7.7176100e-01 1.8632000e-02 6.8640000e-03 3.9230000e-03 3.9230000e-03\n",
      "  4.8690300e-01 1.0002500e-01 1.0000000e+00]\n",
      " [1.0000000e+00 2.4000000e+01 2.4000000e+01 2.2000000e+01 1.8000000e+01\n",
      "  1.6000000e+01 1.3000000e+01 5.7709936e+01 2.3799994e+01 3.3254230e+00\n",
      "  2.3418500e-01 3.9030000e-03 3.9030000e-03 3.9030000e-03 3.9030000e-03\n",
      "  5.2090800e-01 1.4441400e-01 0.0000000e+00]]\n",
      "Class:  [\"b'0'\" \"b'0'\" \"b'1'\" \"b'0'\" \"b'1'\" \"b'1'\" \"b'1'\" \"b'0'\" \"b'1'\" \"b'1'\"]\n"
     ]
    }
   ],
   "source": [
    "diabetic_af = pd.read_csv('messidor_features.csv', na_values='?', index_col = 0)\n",
    "diabetic_af.head()\n",
    "y = diabetic_af.pop('Class').values\n",
    "x_raw = diabetic_af.values\n",
    "print(\"Features: \", x_raw[0:2])\n",
    "print(\"Class: \", y[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and predict using our classifier\n",
    "\n",
    "With a single split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_numbers = list([int(x[2]) for x in y])\n",
    "y_numbers = np.array(y_numbers)\n",
    "y_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 18)\n",
      "(11,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([0,1,1,0])\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 18)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim, out_dim, hidden_units, n_layers, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 2\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_raw, y_numbers, shuffle=True, train_size = 0.1)\n",
    "\n",
    "clf = PerceptronClassifier(len(x_train[0]), 1, 35, 0, learning_rate = 0.03)\n",
    "clf.fit(x_train, y_train, epochs=100)\n",
    "y_pred = clf.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22727399],\n",
       "       [0.22726872],\n",
       "       [0.22730339],\n",
       "       ...,\n",
       "       [0.22726881],\n",
       "       [0.22726882],\n",
       "       [0.78511663]])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search\n",
    "\n",
    "Do grid search with the train set, use the test set for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_folds = 5\n",
    "param_grid ={'activation': ['Sigmoid', 'ReLu', 'TanH'], 'regularisation':['None', 'L2']}\n",
    "\n",
    "# Perform the search\n",
    "tuned_perceptron = GridSearchCV(PerceptronClassifier(), \\\n",
    "                            param_grid, cv=cv_folds, verbose = 2, \\\n",
    "                            n_jobs = -1)\n",
    "cross_val_score(clf, x_train, y_train, cv=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
