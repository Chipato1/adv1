{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP47590: Advanced Machine Learning\n",
    "# Assignment 1: Implementing Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Student 1 Name: Carl Fabian Winkler\n",
    "- Student 1 Number: 20207528\n",
    "- Student 2 Name: David Moreno Boras\n",
    "- Student 2 Number: 21200646"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This is an implementation based on NumPy implementation for a multi-layer perceptron (MLP). In the course Connectionist Computing at UCD we already implemented an MLP from scratch as an assignment. Our projects in C++ and Python can be viewed here:\n",
    "\n",
    "- Carls CPP MLP: https://github.com/Chipato1/NeuralNetCPP\n",
    "- Davids Python MLP: https://github.com/mbdavid2/multi-layer-perceptron\n",
    "\n",
    "With this background, we are focusing on making this project more performant and modular. The goal is to make perceptron easily trainable and very performant. Therefore it should be easy to fit it to new data and selected the hyperparameters. All mathematical transformations should be optimized with NumPy operations. We divided the code into a class \"Layer\" which combines all functions that take part within a layer and a class \"PerceptronClassifier\" which should be accessible from outside and combine the logic to train the estimator. Furthermore, we aim to flexibly switch the batch size and to use different activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.model_selection import train_test_split\\n\\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted, check_random_state\\nfrom sklearn.utils.multiclass import unique_labels\\nfrom sklearn import preprocessing\\n\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.utils import resample'"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn import neural_network\n",
    "from sklearn import ensemble\n",
    "from sklearn import tree\n",
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted, check_random_state\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: The Perceptron Classifier\n",
    "Define the PerceptronClassifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "        n_in            : Number of inputs\n",
    "        n_out           : Number of outputs\n",
    "        activation      : The activation function: Sigmoid, ReLu, TanH\n",
    "        init            : The init, currently only Xavier\n",
    "        output_layer    : If this param is set the deriv. for the activation is not calculated in the backward pass\n",
    "        \n",
    "    Attributes\n",
    "        weights: The weights are learnable parameters of the class \n",
    "        biases : Biases add a static offset for each neuron\n",
    "        \n",
    "    See also\n",
    "        Class Perceptron\n",
    "            - This class uses instances of this class to create an estimator\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out, activation = 'Sigmoid', init = 'Xavier', output_layer = False):\n",
    "        # XW + b = y\n",
    "        self.activation = activation\n",
    "        self.output_layer = output_layer\n",
    "        self.weights = np.zeros((n_in, n_out))\n",
    "        self.biases = np.zeros((n_out))\n",
    "        \n",
    "        # Initialization is done through Xavier\n",
    "        if init == 'Xavier':\n",
    "            var = np.sqrt(6.0 / (n_in + n_out))\n",
    "            for i in range(n_in):\n",
    "                for j in range(n_out):\n",
    "                      self.weights[i,j] = np.float32(np.random.uniform(-var, var))\n",
    "                        \n",
    "        self.d_w = np.zeros(weights.shape)\n",
    "        self.d_b = np.zeros(biases.shape)\n",
    "\n",
    "    #The forward pass consists of an affine transformation of the data and a non-linear activation per neuron\n",
    "    def forward(self, x):\n",
    "        z = x @ self.weights + self.biases\n",
    "        \n",
    "        if self.activation == 'Sigmoid':\n",
    "            out = 1 / (1 + np.exp(-z))\n",
    "        elif self.activation == 'ReLu':\n",
    "            out = np.maximum(z, 0)\n",
    "        elif self.activation == 'TanH':\n",
    "            out = np.tanh(z)\n",
    "        else:\n",
    "            out = z\n",
    "        \n",
    "        self.cache = (x, z)\n",
    "        return out    \n",
    "    \n",
    "    #The backward pass through the activation and the affine transformation\n",
    "    def backward(self, d_out):\n",
    "        inputs, z = self.cache\n",
    "        weight = self.weights\n",
    "        bias = self.biases\n",
    "\n",
    "        if self.output_layer:\n",
    "            d_act = z\n",
    "        if self.activation == 'Sigmoid':\n",
    "            d_act = d_out * (1 / (1 + np.exp(-z))) * (1 - 1 / (1 + np.exp(-z)))\n",
    "        elif self.activation == 'ReLu':\n",
    "            d_act = d_out * (z > 0)\n",
    "            \n",
    "        elif self.activation == 'TanH':\n",
    "            d_act = d_out * np.square(z)\n",
    "        else:\n",
    "            d_act = z\n",
    "   \n",
    "        d_inputs = d_act @ weight.T\n",
    "        self.d_w = inputs.T @ d_act\n",
    "        self.d_b = d_act.sum(axis=0) \n",
    "        \n",
    "        return d_inputs\n",
    "    \n",
    "    # Gradient descent update with weight decay\n",
    "    def update_gd_params(self, lr, weight_decay, batch_size):\n",
    "        #print(\"Weight upd: \", self.d_w)\n",
    "        self.weights = self.weights - lr * (self.d_w + ((2 *weight_decay)/batch_size) * self.weights)\n",
    "        self.biases = self.biases - lr * self.d_b\n",
    "\n",
    "class PerceptronClassifier(BaseEstimator, ClassifierMixin):\n",
    "        \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "        in_dim       : The number of features of the inputs\n",
    "        out_dim      : The number of outputs\n",
    "        hidden_units : The number of neurons in hidden units\n",
    "        n_layers     : The number of extra hiddenlayers, The network always has an input and an output layer!\n",
    "        activation   : The activation function: Sigmoid, ReLu, TanH\n",
    "        learning_rate: The learning rate for gradient descent\n",
    "        weight_decay : The factor for the weight decay / regression term\n",
    "        epochs       : The number of epochs for the training\n",
    "        batch_size   : The number of samples that are passed in an epoch\n",
    "\n",
    "    Attributes\n",
    "        \n",
    "    Notes\n",
    "        This implementation is based on the BaseEstimator class from sklearn.\n",
    "        The number of samples for the fit function can be selected freely.\n",
    "        The batchsize needs to be smaller than the number of samples.\n",
    "        The Inputs should be arrays with the shape (N, in_dim) where N is the number of samples.\n",
    "        \n",
    "    See also\n",
    "        - class Layer\n",
    "            Implementation of the single layers of that perceptron\n",
    "\n",
    "    \"\"\"\n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self, in_dim, out_dim, hidden_units, n_layers, activation = 'Sigmoid', \n",
    "                 learning_rate = 0.01, weight_decay = 0, epochs = 30, batch_size = 16):\n",
    "\n",
    "        \"\"\"Setup a Perceptron classifier - description above\"\"\" \n",
    "        # Initialise class variabels\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.n_layers = n_layers\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_curve = []\n",
    "        self.layers = []     \n",
    "        \n",
    "        # Init all layers, last needs to be marked to avoid backprop. of sigm. activation\n",
    "        self.layers.append(Layer(in_dim, hidden_units, activation, 'Xavier'))\n",
    "        for l in range(n_layers):\n",
    "            self.layers.append(Layer(hidden_units, hidden_units, activation, 'Xavier'))\n",
    "        self.layers.append(Layer(hidden_units, out_dim, activation, 'Xavier', output_layer = True))\n",
    "        \n",
    "    #return the loss_curve     \n",
    "    def get_losscurve(self):\n",
    "        return self.loss_curve\n",
    "    \n",
    "    # Pass some inputs through the network\n",
    "    def forward(self, X):\n",
    "        out = self.layers[0].forward(X)\n",
    "        for layer in self.layers[1:]:\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "    \n",
    "    # Backward pass to go through all layers of the network with the upstream gradient           \n",
    "    def backward(self, in_grad):\n",
    "        i = len(self.layers) - 2 \n",
    "        # d_inputs, _, _ = lay.backward(in_grad)\n",
    "        next_grad = self.layers[i+1].backward(in_grad)\n",
    "        while i >= 0:\n",
    "            next_grad = self.layers[i].backward(next_grad)\n",
    "            i -= 1 \n",
    "            \n",
    "    #compute the BCE loss, add an eps term to avoid numerical unstability       \n",
    "    def bce_loss(self, ground, pred):\n",
    "        eps = np.ones(len(ground))*0.00001\n",
    "        return np.sum(ground* np.log(pred+eps) + (np.ones(len(ground))-ground)* np.log(np.ones(len(ground))-ground+eps))\n",
    "        \n",
    "    # Here we calculate the loss and give back the upstream gradient for BCE-Loss with Sigmoid activation       \n",
    "    def loss_back(self, ground, pred):\n",
    "        #l2_loss = np.sum(np.power(ground-np.squeeze(pred, axis=1), 2)) / len(ground)\n",
    "        loss = bce_loss(ground, pred)\n",
    "        upstream_grad = -np.expand_dims(ground-np.squeeze(pred),axis=1)\n",
    "        return l2_loss, upstream_grad\n",
    "        \n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.epochs):\n",
    "            random_indices = np.random.choice(X.shape[0], size=self.batch_size, replace=False)\n",
    "            x_batch = X[random_indices, :]\n",
    "            y_batch = y[random_indices]\n",
    "            \n",
    "            out = self.forward(x_batch)            \n",
    "            loss, grad = self.loss_back(y_batch, out)\n",
    "            self.loss_curve.append(loss)\n",
    "            \n",
    "            # Backpropagation\n",
    "            self.backward(grad)\n",
    "            \n",
    "            # Update weights and biases\n",
    "            for layer in self.layers:\n",
    "                layer.update_gd_params(self.learning_rate, self.weight_decay, self.batch_size)\n",
    "                \n",
    "        return\n",
    "    \n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict(self, X):\n",
    "        y_pred = self.forward(X)\n",
    "        y_pred_binary = [1 if x >= 0.5 else 0 for x in y_pred]\n",
    "        return np.array(y_pred_binary)\n",
    "    \n",
    "    # The predict_proba function to make a set of predictions for a set of query instances. This returns a set of class distributions.\n",
    "    def predict_proba(self, X):\n",
    "        tmp = self.forward(X)\n",
    "        sum1 = tmp.sum(axis = 1)\n",
    "        out = X.T / sum1\n",
    "        out = out.T\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Diabethic Retinopathy dataset\n",
    "We normalize the data to support the classification. It is a simply min-max transformation to [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "diabetic_af = pd.read_csv('messidor_features.csv', na_values='?', index_col = 0)\n",
    "diabetic_af.head()\n",
    "y = diabetic_af.pop('Class').values\n",
    "x_raw = diabetic_af.values\n",
    "x_norm = NormalizeData(x_raw)\n",
    "y_numbers = list([int(x[2]) for x in y])\n",
    "y_numbers = np.array(y_numbers)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_norm, y_numbers, shuffle=False, train_size = 0.8)\n",
    "\n",
    "x_train_n = NormalizeData(x_train)\n",
    "x_test_n = NormalizeData(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the model\n",
    "To make sure that our model is abled to learn and that it works properly we overfit on a very small part of the data. In this case we reach an accuracy of 100 %. The model therefore clearly has the ability to learn and the capacity to learn and even ovefit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the overfitting-model is:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_overfit = x_train_n[0:15]\n",
    "y_overfit = y_train[0:15]\n",
    "\n",
    "clf = PerceptronClassifier(len(x_train[0]), 1, 30, 1, learning_rate=0.5, epochs=10000,batch_size = 15)\n",
    "clf.fit(x_overfit, y_overfit)\n",
    "y_pred = clf.predict(x_overfit)\n",
    "y_pred_binary = [1 if x >= 0.5 else 0 for x in y_pred]\n",
    "accuracy = metrics.accuracy_score(y_pred_binary, y_overfit)\n",
    "print('Accuracy of the overfitting-model is: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Baseline for the data\n",
    "To have a model that the perceptron can be compared to a baseline model is created here. Therefore a perceptron model and an ensemble of decision tree modesl of the sklearn library is trained to the data. We are trying to reach or even be better than this baseline models in terms of accuracy with the perceptron implemented here. The sklearn MLP reaches an accuracy of 0.73 and the ensemble of decision tree reaches an accuracy of 0.64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Sample SkLearn model: MLP Normalized---------\n",
      "Accuracy: 0.7310195227765727\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.88      0.76       221\n",
      "           1       0.84      0.60      0.70       240\n",
      "\n",
      "    accuracy                           0.73       461\n",
      "   macro avg       0.75      0.74      0.73       461\n",
      "weighted avg       0.76      0.73      0.73       461\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-------Sample SkLearn model: MLP Normalized---------\")\n",
    "my_model = neural_network.MLPClassifier(hidden_layer_sizes=(300, 100), max_iter=1000, solver='adam')\n",
    "my_model = my_model.fit(x_train_n, y_train)\n",
    "\n",
    "# Make a set of predictions for the test data\n",
    "y_pred = my_model.predict(x_test_n)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "# model_valid_accuracy_comparisons[\"MLP\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Sample SkLearn model: Bagging Decision Tree Norm---------\n",
      "Accuracy: 0.6442516268980477\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.88      0.70       221\n",
      "           1       0.79      0.43      0.56       240\n",
      "\n",
      "    accuracy                           0.64       461\n",
      "   macro avg       0.69      0.65      0.63       461\n",
      "weighted avg       0.69      0.64      0.63       461\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-------Sample SkLearn model: Bagging Decision Tree Norm---------\")\n",
    "my_model = ensemble.BaggingClassifier(base_estimator = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf = 50), \\\n",
    "                                      n_estimators=10)\n",
    "my_model = my_model.fit(x_train_n, y_train)\n",
    "\n",
    "# Make a set of predictions for the test data\n",
    "y_pred = my_model.predict(x_test_n)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "# model_valid_accuracy_comparisons[\"MLP\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the perceptron\n",
    "Now that the model can overfit some samples and two baseline models for the problem we start training the implemented perceptron on the data. We do a grid search over a huge space of different hyperparameters that the model supports. This keeps the computer busy for quite some time :) We do two grid searches here and take the best model. In the second grid search, we build upon the configuration that has shown to be good in the first search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 432 candidates, totalling 864 fits\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=   7.6s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=   8.1s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=   8.3s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=   8.6s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=0; total time=   9.6s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  10.1s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=0; total time=   9.6s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=   9.5s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=   9.3s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=0; total time=   9.1s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=   9.0s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=   9.5s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  12.0s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  11.9s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  11.8s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  12.1s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  12.1s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  12.5s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  12.7s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  12.5s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  12.4s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  12.5s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  13.1s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  12.5s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  15.5s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  15.2s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  15.1s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  15.2s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  14.7s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  14.7s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  15.1s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  15.7s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  15.3s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  15.5s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  15.0s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  14.7s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  11.7s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  11.4s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  11.4s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  11.6s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  11.7s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  12.1s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  12.3s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  12.2s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  12.1s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  11.8s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  12.1s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  11.5s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  15.6s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  15.9s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  16.1s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  15.8s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  16.0s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  17.3s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  16.3s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  15.2s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  15.0s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  15.6s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  15.3s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  15.7s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  20.0s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  21.5s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  19.7s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  19.7s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  19.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=   7.4s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=   7.9s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=0; total time=   8.6s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=   8.3s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=   8.7s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=0; total time=   9.0s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=   9.5s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=   9.4s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=0; total time=   9.7s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=   9.1s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=   9.1s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=0; total time=   9.1s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  12.1s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  12.0s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  12.0s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  12.8s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  12.9s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  12.7s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  12.9s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  12.4s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  11.8s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  11.6s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  11.9s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  11.8s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  15.5s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  15.2s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  14.8s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  15.4s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  15.0s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  14.6s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  14.9s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  15.7s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  15.1s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  16.0s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  16.3s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  15.7s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  11.9s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  12.0s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  12.3s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  12.4s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  12.7s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  12.6s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  11.8s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  11.6s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  11.6s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  11.5s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  11.4s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  11.6s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  15.6s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  16.1s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  15.4s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  15.3s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  15.7s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  16.1s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  16.8s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  15.7s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  15.9s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  15.6s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  15.6s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  15.8s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  19.2s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  21.6s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  19.6s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  19.4s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  19.6s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=   7.6s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=0; total time=   8.4s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=   8.9s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=0; total time=   8.9s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=   9.8s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=   9.4s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=0; total time=   9.8s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=   9.8s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=0; total time=   9.2s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=   9.0s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=   8.8s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=0; total time=   8.9s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  11.8s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  11.9s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  11.9s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  12.1s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  12.9s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  12.3s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  12.1s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  12.0s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  11.9s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  11.8s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  12.2s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  11.9s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  15.0s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  15.7s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  15.7s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  16.1s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  16.0s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  15.6s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  16.0s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  15.2s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  15.2s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  15.8s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  14.6s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  14.6s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  11.7s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  11.6s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  11.9s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  11.8s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  11.9s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  11.8s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  11.9s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  12.4s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  11.8s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  11.6s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  11.7s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  12.3s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  16.3s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  15.8s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  15.5s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  15.6s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  15.7s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  15.8s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  16.6s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  15.4s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  15.5s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  15.6s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  15.4s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  15.9s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  20.2s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  21.2s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  20.1s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  20.1s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  20.3s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=   7.6s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=   8.2s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=   8.3s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=   8.8s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=0; total time=   9.2s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  10.0s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=   9.4s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=0; total time=   9.6s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=   9.1s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=   8.8s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=0; total time=   8.8s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=   9.0s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  11.8s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  11.8s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  12.0s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  12.2s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  12.2s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  12.2s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  12.0s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  12.5s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  12.2s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  12.3s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  12.9s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  12.2s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  14.6s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  15.1s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  14.7s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  14.9s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  15.4s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  15.2s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  15.3s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  15.5s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  15.6s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  15.5s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  15.6s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  15.2s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  11.9s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  12.0s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  11.4s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  11.9s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  12.0s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  11.7s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  12.3s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  12.4s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  11.7s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  11.5s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  11.4s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  12.1s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  15.9s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  15.6s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  15.6s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  15.5s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  15.5s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  16.7s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  19.3s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  16.1s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  16.0s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  16.2s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  16.0s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  15.7s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  19.3s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  20.5s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  20.0s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  20.2s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  19.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=0; total time=   7.5s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=0; total time=   8.6s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=   9.4s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=   9.2s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=   9.2s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=0; total time=   9.2s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=   8.8s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=   9.0s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=   9.2s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=0; total time=   9.2s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=   9.2s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=   9.4s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  12.2s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  12.2s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  12.3s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  12.5s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  12.6s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  12.3s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  12.5s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  12.0s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  11.9s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  12.1s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  12.0s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  12.1s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  15.2s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  15.9s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  15.9s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  15.7s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  15.9s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  16.1s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  16.2s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  15.9s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  15.6s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  16.1s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  15.1s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  15.3s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  12.1s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  11.6s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  11.5s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  12.2s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  11.8s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  12.0s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  11.9s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  11.7s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  11.7s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  11.7s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  11.8s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  11.9s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  15.8s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  15.7s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  15.4s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  15.4s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  15.5s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  16.9s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  16.2s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  15.5s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  15.5s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  15.1s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  15.1s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  15.4s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  20.1s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  22.1s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  19.8s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  19.7s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  20.0s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=   7.8s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=   8.5s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=0; total time=   8.5s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=0; total time=   8.5s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=   8.9s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=   8.8s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=   9.0s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=   9.0s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=0; total time=   9.2s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=   8.8s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=   8.9s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=0; total time=   8.9s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  12.0s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  12.2s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  12.1s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  12.2s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  12.4s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  12.7s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  11.8s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  12.2s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  12.2s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  12.2s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  12.3s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  12.4s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=  15.5s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=  15.6s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=0; total time=  15.8s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  15.5s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  15.9s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  15.9s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  15.9s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  15.4s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  15.6s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  15.9s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  15.7s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  15.8s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  12.2s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  11.8s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  11.7s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  11.7s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  11.7s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  12.0s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  13.1s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  12.0s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  11.8s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  11.8s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  11.8s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  12.3s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  16.5s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  16.9s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  16.4s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  15.8s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  16.3s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  17.6s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  15.4s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  15.1s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  15.1s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  15.1s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  15.1s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  15.9s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  20.2s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  21.5s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  20.3s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  21.0s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  19.7s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-05; total time=   7.4s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=   8.0s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=0; total time=   8.7s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=   8.3s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-06; total time=   9.1s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=0; total time=   9.5s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=   9.6s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=0; total time=   9.7s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=   9.1s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=   9.2s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=0; total time=   9.3s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=   9.3s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  12.1s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  12.1s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  12.1s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  12.3s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  12.7s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  12.2s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  12.0s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  12.1s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-05; total time=  11.9s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=5e-06; total time=  12.5s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=0; total time=  12.3s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  12.4s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  16.0s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  16.0s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  15.9s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  15.9s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  16.0s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  15.8s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  16.2s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  15.7s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  16.0s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  15.9s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  15.1s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  15.3s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  12.5s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  12.0s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  11.8s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  11.9s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  11.7s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  11.9s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  11.8s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  11.8s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  12.0s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  11.6s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  11.9s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  11.6s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  15.4s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  16.1s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  15.6s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  15.7s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  16.2s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  18.2s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  15.7s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  15.3s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  15.6s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  15.6s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  15.7s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  15.9s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  20.5s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  21.2s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  19.3s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  20.1s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  19.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.1, n_layers=0, weight_decay=0; total time=   8.0s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=   8.7s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=0; total time=   9.2s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=   9.3s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=   9.7s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  10.1s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=   9.7s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  10.0s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=0; total time=   9.0s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=   9.0s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=   8.9s\n",
      "[CV] END batch_size=32, epochs=15000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=0; total time=   9.4s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  12.0s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  11.9s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  11.9s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  12.3s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  12.3s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  12.5s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  12.5s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  12.4s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  12.1s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  12.4s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  11.9s\n",
      "[CV] END batch_size=32, epochs=20000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  12.3s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  15.0s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  15.3s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  15.3s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  15.4s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  15.4s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  15.3s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  15.9s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  15.4s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  15.8s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.1, n_layers=0, weight_decay=0; total time=  15.7s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-05; total time=  16.3s\n",
      "[CV] END batch_size=32, epochs=25000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=5e-06; total time=  15.7s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  12.7s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  11.8s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  12.3s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  12.0s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  12.0s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  12.4s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  12.4s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  12.2s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  11.9s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  11.5s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  11.9s\n",
      "[CV] END batch_size=64, epochs=15000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  11.9s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  15.7s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  16.5s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  15.7s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  15.8s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  16.0s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=70, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  17.5s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  15.6s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  15.7s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=100, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  16.1s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  15.4s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  15.7s\n",
      "[CV] END batch_size=64, epochs=20000, hidden_units=120, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  15.6s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  20.2s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  21.5s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=50, learning_rate=0.001, n_layers=0, weight_decay=0; total time=  19.9s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=70, learning_rate=0.01, n_layers=0, weight_decay=5e-05; total time=  20.1s\n",
      "[CV] END batch_size=64, epochs=25000, hidden_units=70, learning_rate=0.005, n_layers=0, weight_decay=5e-06; total time=  19.9s\n",
      "Accuracy of the tuned model is:  0.7114967462039046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'epochs': 25000,\n",
       " 'hidden_units': 100,\n",
       " 'learning_rate': 0.1,\n",
       " 'n_layers': 0,\n",
       " 'weight_decay': 5e-05}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = [15000,20000,25000]\n",
    "hidden_units = [50, 70, 100, 120]\n",
    "learning_rates = [0.1, 0.01, 0.005, 0.001]\n",
    "batch_size = [32, 64,128]\n",
    "hidden_layers = [0]\n",
    "weight_decay = [0.00005,0.000005, 0]\n",
    "\n",
    "param_grid ={'epochs': epochs, \\\n",
    "             'n_layers': hidden_layers, \\\n",
    "             'batch_size': batch_size, \\\n",
    "             'hidden_units': hidden_units, \\\n",
    "             'learning_rate': learning_rates, \\\n",
    "             'weight_decay': weight_decay}\n",
    "\n",
    "cv_folds = 2\n",
    "\n",
    "tuned_model = GridSearchCV(PerceptronClassifier(in_dim = len(x_train_n[0]), out_dim = 1, hidden_units = 30, \\\n",
    "                                                n_layers = 1, learning_rate=0.5, activation='Sigmoid'), \\\n",
    "                                param_grid, cv=cv_folds, verbose = 2, \\\n",
    "                                return_train_score=True, n_jobs = -1)\n",
    "\n",
    "tuned_model.fit(x_train_n, y_train)\n",
    "y_pred = tuned_model.predict(x_test_n)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_pred, y_test)\n",
    "print('Accuracy of the tuned model is: ', accuracy)\n",
    "display(tuned_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 144 candidates, totalling 288 fits\n"
     ]
    }
   ],
   "source": [
    "epochs = [40000, 50000]\n",
    "hidden_units = [75, 100, 125]\n",
    "learning_rates = [0.01,0.03,0.005]\n",
    "batch_size = [32, 64]\n",
    "hidden_layers = [0,1]\n",
    "weight_decay = [0.000005,0]\n",
    "\n",
    "param_grid ={'epochs': epochs, \\\n",
    "             'n_layers': hidden_layers, \\\n",
    "             'batch_size': batch_size, \\\n",
    "             'hidden_units': hidden_units, \\\n",
    "             'learning_rate': learning_rates, \\\n",
    "             'weight_decay': weight_decay}\n",
    "\n",
    "cv_folds = 2\n",
    "\n",
    "tuned_model = GridSearchCV(PerceptronClassifier(in_dim = len(x_train_n[0]), out_dim = 1, hidden_units = 30, \\\n",
    "                                                n_layers = 1, learning_rate=0.5, activation='Sigmoid'), \\\n",
    "                                param_grid, cv=cv_folds, verbose = 2, \\\n",
    "                                return_train_score=True, n_jobs = -1)\n",
    "\n",
    "tuned_model.fit(x_train_n, y_train)\n",
    "y_pred = tuned_model.predict(x_test_n)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_pred, y_test)\n",
    "print('Accuracy of the tuned model is: ', accuracy)\n",
    "display(tuned_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do this again to train an optimal model and investigate the influence of the random-intialization.\n",
    "# This model is performed several times\n",
    "model = PerceptronClassifier(in_dim = len(x_train_n[0]), \n",
    "                             out_dim = 1, \\\n",
    "                             hidden_units = 100, \\\n",
    "                             batch_size = 64,\\\n",
    "                             n_layers = 0, \\\n",
    "                             epochs = 25000,\\\n",
    "                             weight_decay = 5e-05, \\\n",
    "                             learning_rate= 0.1, \\\n",
    "                             activation='Sigmoid')\n",
    "\n",
    "\n",
    "\n",
    "model.fit(x_train_n, y_train)\n",
    "y_pred = model.predict(x_test_n)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_pred, y_test)\n",
    "print('Accuracy of the tuned model is: ', accuracy)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss-curve\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot(111)\n",
    "loss = model.get_losscurve()\n",
    "plt.plot(loss)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 & 4: Add Different Activations & Regularisation\n",
    "\n",
    "Reimplement the PerceptronClassifier class adding an activation function option and L2 regularisation. Our model is structured modulary. This way we could directly introduce the capability to use regularisation and different activation functions. However for different activation function we only support the L2-Loss and not BCE-Loss which is less suited for this kind of problem and the task was deleted from this assignment. The implementation can be found in the class and we extensively evaluated the parts before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Reflect on the Performance of the Different Models Evaluated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of models\n",
    "\n",
    "The best model we trained reached an accuracy of 0.72 on the test data. This is about 0.02 worse than the MLP model from sklearn. Although we tried countless configurations we couldn't beat sklearn. However, this implementation performs significantly better than the ensemble of decision trees. This can be explained by a higher capability of the MLP to generalize on complex interrelationships of the data. The fact that the MLP from sklearn performs better can be explained by the usage of a better optimization algorithm. While this implementation uses a mini-batch vanilla gradient descent optimization method, the sklearn-model uses ADAM which allows it to reach better minima significantly faster. Furthermore, it is likely that our implementations depend more on the initialization of the weights. Furthermore, all models tend to assign more ones then zeros. The training set is only slighlty misbalanced. This can therefore most likely not explain the effect.  In conclusion, our model reached a good accuracy and could be configured freely and the training is reasonably fast.\n",
    "\n",
    "#### Training of the model and hyperparameters\n",
    "\n",
    "During the training of the model, we found that most models first adapt to classifying all samples as 0 or 1. This can be seen in the loss curve as an initial plateau. Some models do not leave this plateau to the end of the epoch which is problematic. It showed that using a smaller batch size can help to overcome this plateau faster.\n",
    "\n",
    "The hyperparameters of this model are highly dependent on each other. This makes it more difficult to find a good set of parameters. When the batch size gets smaller it tends to be beneficial to also decrease the learning rate. A smaller learning rate then tends to work well with an increased number of epochs and so on. Therefore, doing a grid search over a huge configuration, space really helped to find a good set of parameters. This is computationally intensive and therefore takes some time. On average a model to predict diabetes needs like 10s to train in our testing environment.\n",
    "\n",
    "The weight decay has proven to be beneficial but with a very small scaling value. In the loss curve, it could be noticed that the weight decay tends to decrease the oscillations in the loss. In general, it seems that the influence of the weight decay is especially helpful when models get really high accuracy and get in danger to overfit the training data.\n",
    "\n",
    "Our final model and its configuration can be seen in the cells above. Furthermore, we show the loss curve on the training data to illustrate how the model is trained. We use a relatively low batch size and train for many epochs. After a short swing to predicting 1 for all values, our model starts to predict the values of the test set more and more precisely.\n",
    "\n",
    "#### Summary\n",
    "In conclusion, the model can learn the diabetes dataset well and the model with the best configurations could reach an accuracy of 0.72 for the prediction on the test set. The weight decay has also proven to be useful to prevent overfitting in some models. In general, the task of finding a good hyperparameter set is very tedious and takes a lot of time. \n",
    "\n",
    "#### Limitations and Outlook\n",
    "For further development of this implementation, we will try to implement a more sophisticated optimization method like ADAM. This should improve the convergence towards a good parameter-set. The major limitation of this implementation is that it still takes around 10s to train an average model for our problem. This is significantly slower than with libraries like Pytorch. To improve this it could be considered to do the matrix operations on the GPU of the computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
